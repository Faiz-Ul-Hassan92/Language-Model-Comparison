Experiment ID: EXP-001 
Date: 23/11/2025
Hypothesis: Increasing epochs from X  to  Y will improve ? 
Configuration: - Model: LSTM - Optimizer: RMSprop - Changed Parameter: epochs =40 - Other Parameters: [list baseline values] 
Results: - Baseline Perplexity: 568.91 - Experimental Perplexity: XXX.XX - Training Time: XX.XX minutes - Improvement: +/- X.XX points 
Observation: 
[What happened? Better or worse? Why?] 
Conclusion: 
[Accept or reject hypothesis? What did you learn?]



-- last token embedding
-- remove early stop and dropout to 0.5
-- dropout to 0.3 to make sure i am not halting learning, proved, 0.5 acts good
-- epochs to 50, model still can't capture, time to make it complex and rms is abrupt so smoothen it 